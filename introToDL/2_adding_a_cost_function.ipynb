{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d84d95",
   "metadata": {},
   "source": [
    "## Adding Cost Function and Optimization to Neural Networks\n",
    "\n",
    "For this exercise, your task is to add a Cost Function and Optimizer to the neural network you built in the last exercise. You will need to figure out what is the correct cost function and optimizer to use for your neural network architecture. Here are the steps you need to do:\n",
    "\n",
    "1. Complete the `create_model()` function. You can either create a new network or use the network you built in the previous exercise\n",
    "2. Add your cost function and optimizer\n",
    "\n",
    "**Note**: It may take 5 - 10 minutes to download the data sets. \n",
    "\n",
    "In case you get stuck, you can look at the solution below.\n",
    "\n",
    "### Try It Out!\n",
    "- Change the parameters of your optimizer and for your network. How does your model accuracy change? These values are called hyperparameters and they can change the performance of our model. In a later lesson, we will learn how to automatically search for hyperparameters that give the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12c8a4",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da052349",
   "metadata": {},
   "source": [
    "### Download and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5aee4",
   "metadata": {},
   "source": [
    "#### Proprocess data\n",
    "Here we are not actually performing transformations but rather insntancig the tranformation functions that will be applied to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance preprocessing/transformation functions\n",
    "# Perform data augmentation with the training dataset\n",
    "training_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "testing_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b34bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load data\n",
    "batch_size = 64\n",
    "\n",
    "trainset = datasets.MNIST('data/', download=True, train=True, transform=training_transform)\n",
    "testset = datasets.MNIST('data/', download=True, train=False, transform=testing_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717df63",
   "metadata": {},
   "source": [
    "### Create and Instance a NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    '''Creates a PyTorch NN using the Sequential API'''\n",
    "\n",
    "    input_size = 784\n",
    "    output_size = 10\n",
    "\n",
    "    model = nn.Sequential(nn.Linear(input_size, 128),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(128, 64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(64, output_size),\n",
    "                          nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae169175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f87086",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90284a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train(model, train_loader, cost, optimizer, epoch):\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        # 1. Loop through data\n",
    "        for data, target in train_loader:\n",
    "            # Reshape data\n",
    "            data = data.view(data.shape[0], -1)\n",
    "             # 4. Optimizer zero grad\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the variables it will update (which are the learnable\n",
    "            # weights of the model). This is because by default, gradients are\n",
    "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "            # is called.\n",
    "            optimizer.zero_grad()\n",
    "            # 2. Forward pass\n",
    "            pred = model(data)\n",
    "            # 3. Compute loss\n",
    "            loss = cost(pred, target)\n",
    "            running_loss+=loss\n",
    "            # 5. Backpropagation: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # 6. Update the weights using gradient descent\n",
    "            optimizer.step()\n",
    "            # Get the index of the max log-probabilty\n",
    "            pred = pred.argmax(dim=1, keepdim=True)\n",
    "            # Count the number of correct predictions\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        print(f\"Epoch {e}: Loss {running_loss/len(train_loader.dataset)}, Accuracy {100*(correct/len(train_loader.dataset))}%\")\n",
    "\n",
    "# Define testing loop\n",
    "def test(model, test_loader):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        # Loope through data in batches\n",
    "        for data, target in test_loader:\n",
    "            data = data.view(data.shape[0], -1) # Reshape data\n",
    "            output = model(data) \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    print(f'Test set: Accuracy: {correct}/{len(test_loader.dataset)} = {100*(correct/len(test_loader.dataset))}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model configs\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Set model Hyperparameters\n",
    "epochs = 10\n",
    "\n",
    "train(model,\n",
    "      train_loader,\n",
    "      loss_fn,\n",
    "      optimizer,\n",
    "      epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
