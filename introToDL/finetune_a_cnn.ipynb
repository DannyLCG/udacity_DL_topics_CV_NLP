{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Convolutional Neural Network\n",
    "\n",
    "In this exercise, you will have to finetune a pretrained CNN model on the CIFAR10 dataset. The data loading and model testing logic are already included in your code. You will have to create the model and the training loop.\n",
    "\n",
    "**In this workspace you have GPU to help train the model but it is best practice to DISABLE it while writing code and only ENABLE it when you are training.** \n",
    "\n",
    "Here are the steps you need to do to complete this exercise:\n",
    "\n",
    "1. Finish the `create_model()` function. You should use a pretrained model. You are free to choose any pre-trained model that you want to use. \n",
    "2. Finish the `train()` function. This function should validate the accuracy of the model during the training stage. You should stop the training when this validation accuracy stops increasing.\n",
    "3. Save all your work and then **ENABLE** the GPU\n",
    "4. Run the file to make sure that the model is training properly.\n",
    "5. If it works, remember to **DISABLE** the GPU before moving to the next page. \n",
    "\n",
    "In case you get stuck, you can look at the solution by clicking the jupyter symbol at the top left and navigating to `finetune_a_cnn_solution.py`.\n",
    "\n",
    "## Try It Out!\n",
    "- See how your accuracy changes when using other pre-trained models.\n",
    "- Play around with the number of layers and neurons in your model. How does the accuracy change? How long does it take to train the model?\n",
    "- Can you create the same network in TensorFlow as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "# Define data transformations\n",
    "training_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "testing_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Download datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=training_transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=testing_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and instance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Instance the pre-trained model from the 'models' module\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Freeze the conv layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Add fc layer using the Sequential API\n",
    "    num_feats = model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Linear(num_feats, 10))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run a pretrained model we might need the use of a GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Running on device {device}')\n",
    "\n",
    "# Instance the model\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, validation_loader, epochs, loss_fn, optimizer, patience=3, baseline=1e-6):\n",
    "    best_loss = baseline\n",
    "    image_dataset = {'train':train_loader, 'valid':validation_loader}\n",
    "    loss_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0.0 \n",
    "            correct = 0 # To calculate epoch accuracy\n",
    "            total = 0 #EXPLAIN\n",
    "            # 1. Loop through data\n",
    "            for data, target in image_dataset[phase]:\n",
    "                # Move data and target to the same device as model\n",
    "                data, target = data.to(next(model.parameters()).device), target.to(next(model.parameters()).device) #EXPLAIN next()\n",
    "\n",
    "                # 2. Zero all gradients\n",
    "                optimizer.zero_grad()\n",
    "                # 3. Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    pred = model(data)\n",
    "                    # 4 Compute loss\n",
    "                    loss = loss_fn(pred, target)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        # 5 Backpropagation\n",
    "                        loss.backward()\n",
    "                        # 6 Update weights\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Update epoch los...\n",
    "                epoch_loss += loss.item() * data.size(0)\n",
    "                _, predicted = torch.max(pred, 1) #EXPLAIN\n",
    "                correct += (predicted == target).sum().item()\n",
    "                total += target.size(0)\n",
    "\n",
    "            epoch_loss = epoch_loss / len(image_dataset[phase].dataset)\n",
    "            epoc_acc = correct / total * 100\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f}, Accuracy: {epoc_acc:.2f}%')\n",
    "\n",
    "            # Save the model if validation loss decreases\n",
    "            if phase == 'valid' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                loss_counter = 0\n",
    "                #torch.save(model.state_dict(), 'best_model.pth')\n",
    "            elif phase == 'valid':\n",
    "                loss_counter += 1\n",
    "\n",
    "            # Early stopping\n",
    "            if loss_counter >= patience:\n",
    "                print(f'Early stopping at {epoch} epoch.')\n",
    "\n",
    "                return model\n",
    "            \n",
    "    return model\n",
    "                \n",
    "\n",
    "def test(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        # Move data to the same device as the model\n",
    "        inputs, labels = inputs.to(next(model.parameters()).device), labels.to(next(model.parameters()).device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    total_loss = running_loss / len(test_loader)\n",
    "    total_acc = running_corrects / len(test_loader)\n",
    "    print(f'Testing Loss: {total_loss}, Testing Accuracy: {100 * total_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model configs\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=lr)\n",
    "\n",
    "# Set model hyperparams\n",
    "\n",
    "train(model, train_loader, test_loader, epochs, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to Disable GPU when you are done training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
